{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pathlib\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "DATA_DIR = \"facial_expressions/data\"\n",
    "IMAGE_DIR = \"facial_expressions/images\"\n",
    "\n",
    "data = pd.read_csv(f'{DATA_DIR}/legend.csv')\n",
    "data = data.head(300)\n",
    "# data = pd.read_csv(f'{DATA_DIR}/500_picts_satz.csv', names=['user', 'image', 'emotion'])\n",
    "data['emotion'] = data['emotion'].str.lower()\n",
    "data.rename(columns={'image': 'path'}, inplace=True)\n",
    "data['image'] = [torch.unsqueeze(torch.Tensor(plt.imread(f'{IMAGE_DIR}/{path}').astype('float32')), 0) for path in data['path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.id</th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "      <th>image</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>302</td>\n",
       "      <td>facial-expressions_2868588k.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor([203., 201., 197.]), tensor([203., 2...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user.id                             path  emotion  \\\n",
       "count      300                              300      300   \n",
       "unique       3                              300        7   \n",
       "top        302  facial-expressions_2868588k.jpg  neutral   \n",
       "freq       295                                1      182   \n",
       "\n",
       "                                                    image          shape  \n",
       "count                                                 300            300  \n",
       "unique                                                300              4  \n",
       "top     [[[tensor([203., 201., 197.]), tensor([203., 2...  (1, 350, 350)  \n",
       "freq                                                    1            296  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['shape'] = [img.shape for img in data['image']]\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.id</th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "      <th>image</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwdii</td>\n",
       "      <td>Aaron_Eckhart_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor(9.), tensor(9.), tensor(9.), tensor(...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>302</td>\n",
       "      <td>Aaron_Guiel_0001.jpg</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[[[tensor(10.), tensor(10.), tensor(10.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>302</td>\n",
       "      <td>Aaron_Patterson_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor(92.), tensor(92.), tensor(92.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>302</td>\n",
       "      <td>Aaron_Peirsol_0001.jpg</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[[[tensor(125.), tensor(125.), tensor(118.), t...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>302</td>\n",
       "      <td>Aaron_Peirsol_0002.jpg</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[[[tensor(82.), tensor(82.), tensor(83.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>302</td>\n",
       "      <td>Alex_Cabrera_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor(47.), tensor(47.), tensor(49.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>302</td>\n",
       "      <td>Alex_Cejka_0001.jpg</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[[[tensor(18.), tensor(18.), tensor(17.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>302</td>\n",
       "      <td>Alex_Corretja_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor(47.), tensor(47.), tensor(46.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>302</td>\n",
       "      <td>Alex_Ferguson_0001.jpg</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[[[tensor(89.), tensor(90.), tensor(91.), tens...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>302</td>\n",
       "      <td>Alex_Gonzalez_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[[[tensor(147.), tensor(147.), tensor(146.), t...</td>\n",
       "      <td>(1, 350, 350)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user.id                      path    emotion  \\\n",
       "4     dwdii    Aaron_Eckhart_0001.jpg    neutral   \n",
       "5       302      Aaron_Guiel_0001.jpg  happiness   \n",
       "6       302  Aaron_Patterson_0001.jpg    neutral   \n",
       "7       302    Aaron_Peirsol_0001.jpg  happiness   \n",
       "8       302    Aaron_Peirsol_0002.jpg  happiness   \n",
       "..      ...                       ...        ...   \n",
       "295     302     Alex_Cabrera_0001.jpg    neutral   \n",
       "296     302       Alex_Cejka_0001.jpg  happiness   \n",
       "297     302    Alex_Corretja_0001.jpg    neutral   \n",
       "298     302    Alex_Ferguson_0001.jpg  happiness   \n",
       "299     302    Alex_Gonzalez_0001.jpg    neutral   \n",
       "\n",
       "                                                 image          shape  \n",
       "4    [[[tensor(9.), tensor(9.), tensor(9.), tensor(...  (1, 350, 350)  \n",
       "5    [[[tensor(10.), tensor(10.), tensor(10.), tens...  (1, 350, 350)  \n",
       "6    [[[tensor(92.), tensor(92.), tensor(92.), tens...  (1, 350, 350)  \n",
       "7    [[[tensor(125.), tensor(125.), tensor(118.), t...  (1, 350, 350)  \n",
       "8    [[[tensor(82.), tensor(82.), tensor(83.), tens...  (1, 350, 350)  \n",
       "..                                                 ...            ...  \n",
       "295  [[[tensor(47.), tensor(47.), tensor(49.), tens...  (1, 350, 350)  \n",
       "296  [[[tensor(18.), tensor(18.), tensor(17.), tens...  (1, 350, 350)  \n",
       "297  [[[tensor(47.), tensor(47.), tensor(46.), tens...  (1, 350, 350)  \n",
       "298  [[[tensor(89.), tensor(90.), tensor(91.), tens...  (1, 350, 350)  \n",
       "299  [[[tensor(147.), tensor(147.), tensor(146.), t...  (1, 350, 350)  \n",
       "\n",
       "[296 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_shape_data = data[data['shape'] == (1, 350, 350)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_img = torch.Tensor(data['image'])\n",
    "# tensor_label = torch.Tensor(data['emotion'])\n",
    "# convert_tensor = transforms.ToTensor()\n",
    "# convert_tensors = np.vectorize(convert_tensor)\n",
    "# ts = [convert_tensor(img) for img in images]\n",
    "# tensor_big = torch.Tensor(ts)\n",
    "# torch.unsqueeze(torch.Tensor(images[0]), 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class FacialExpression(Dataset):\n",
    "  def __init__(self, annotations_file=f'{DATA_DIR}/legend.csv', img_dir=IMAGE_DIR, transform=None, target_transform=None):\n",
    "    self.img_dir = img_dir\n",
    "    self.data = same_shape_data\n",
    "    self.classes = list(self.data['emotion'].unique())\n",
    "    self.data['class'] = [self.classes.index(e) for e in self.data['emotion']]\n",
    "    self.img_labels = self.data['class']\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.img_labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data['image'][idx]\n",
    "    label = self.img_labels[idx]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14728/3822882487.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['class'] = [self.classes.index(e) for e in self.data['emotion']]\n"
     ]
    }
   ],
   "source": [
    "trainset = FacialExpression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torchvision\n",
    "\n",
    "# # functions to show an image\n",
    "\n",
    "\n",
    "# def imshow(img):\n",
    "#     # img = img / 2 + 0.5     # unnormalize\n",
    "#     # npimg = img.numpy()\n",
    "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(' '.join(f'{labels[j]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(112896, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo 189yo 76\n",
      "\n",
      "po 76po 189\n",
      "\n",
      "yo 128yo 83\n",
      "\n",
      "po 128po 83\n",
      "\n",
      "yo 93yo 221\n",
      "\n",
      "po 93po 221\n",
      "\n",
      "yo 240yo 133\n",
      "\n",
      "po 240po 133\n",
      "\n",
      "yo 92yo 116\n",
      "\n",
      "po 92po 116\n",
      "\n",
      "yo 245yo 49\n",
      "\n",
      "po 245po 49\n",
      "\n",
      "yo 38\n",
      "po 38\n",
      "yo 145yo 105\n",
      "\n",
      "po 145po 105\n",
      "\n",
      "yo 91\n",
      "po 91\n",
      "yo 104\n",
      "po 104\n",
      "yo 295\n",
      "po 295\n",
      "yo 173\n",
      "po 173\n",
      "yo 127\n",
      "po 127\n",
      "yo 60\n",
      "po 60\n",
      "yo 283\n",
      "po 283\n",
      "yo 155\n",
      "po 155\n",
      "yo 39\n",
      "po 39\n",
      "yo 40\n",
      "po 40\n",
      "yo 293\n",
      "po 293\n",
      "yo 146\n",
      "po 146\n",
      "yo 3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "yo 73\n",
      "po 73\n",
      "yo 86\n",
      "po 86\n",
      "yo 135\n",
      "po 135\n",
      "yo 291\n",
      "po 291\n",
      "yo 26\n",
      "po 26\n",
      "yo 87\n",
      "po 87\n",
      "yo 267\n",
      "po 267\n",
      "yo 153\n",
      "po 153\n",
      "yo 234\n",
      "po 234\n",
      "yo 124\n",
      "po 124\n",
      "yo 107\n",
      "po 107\n",
      "yo 159\n",
      "po 159\n",
      "yo 81\n",
      "4\n",
      "5\n",
      "po 81"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3803, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2263, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2273, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_14728/3822882487.py\", line 21, in __getitem__\n    image = self.data['image'][idx]\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 981, in __getitem__\n    return self._get_value(key)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1089, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):  \u001b[39m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m   \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      6\u001b[0m     \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3803, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2263, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 2273, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_14728/3822882487.py\", line 21, in __getitem__\n    image = self.data['image'][idx]\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 981, in __getitem__\n    return self._get_value(key)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1089, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/home/tjhu/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "yo 259\n",
      "po 259\n",
      "yo 228\n",
      "po 228\n",
      "yo 143\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "  running_loss = 0.0\n",
    "  for i, batch in enumerate(trainloader, 0):\n",
    "    print(i)\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = batch\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "      running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 536, 347, 3] to have 3 channels, but got 536 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet18, ResNet18_Weights\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m resnet18(weights\u001b[39m=\u001b[39mResNet18_Weights\u001b[39m.\u001b[39mDEFAULT)\n\u001b[0;32m----> 3\u001b[0m p \u001b[39m=\u001b[39m model(data[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 536, 347, 3] to have 3 channels, but got 536 channels instead"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "p = model(data['image'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
